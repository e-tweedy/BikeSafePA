{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b4cfcfb-71e4-4d47-8806-c62fbd20f1c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## BikeSaferPA: predicting outcomes for cyclists using Pennsylvania crash data, 2002-2021\n",
    "\n",
    "**For best rendering results, view this notebook in [NBViewer](https://nbviewer.org/github/e-tweedy/BikeSaferPA/blob/main/1_BikeSaferPA_data.ipynb).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6915db-3037-45a2-8907-f69da82374a3",
   "metadata": {},
   "source": [
    "## Introduction to the BikeSaferPA project\n",
    "\n",
    "Which factors most dramatically impact a cyclist's risk of suffering serious injury or death in a crash?\n",
    "\n",
    "Can we predict the severity of a cyclist's outcome based on these factors?\n",
    "\n",
    "To address these questions I've build **BikeSaferPA**, a machine learning model which I designed to predict whether a cyclist in a crash in Pennsylvania will suffer serious injury or fatality as a result.  I focused on a publically accessible dataset of crash records in the state during the period of 2002-2021, made available by Pennsylvania Department of Transportation (PENNDOT).  This series of Jupyter notebooks will descibe this process.\n",
    "\n",
    "### Project goals\n",
    "\n",
    "I focused on cyclists involved in crash incidents appearing in this dataset.  The goals of this project are:\n",
    "1. Examine and visualize the prevalence of cyclist-involved crash incidents both across the state and locally, and how the prevalence has changed during 2002-2021.\n",
    "2. Examine and visualize the prevalence of various crash incident factors among cyclist-involved crash incidents.\n",
    "3. Determine the factors in a bicycle-involved crash incident which contribute to increased risk of serious cyclist injury or cyclist fatality.\n",
    "4. Design a machine learning model BikeSaferPA to predict serious cyclist injury or fatality based on various crash features available to use in this dataset.\n",
    "5. Evaluate the performance of BikeSaferPA and assess the importance of various crash factors on its predictions.\n",
    "\n",
    "### Broad dataset description\n",
    "\n",
    "PENNDOT publishes a publically accessible dataset containing records regarding all crashes in the state involving motor vehicles, cyclists, pedestrians, etc.  The Pennsylvania dataset is somewhat unique among crash datasets in that it includes significantly more data features about bicycles and cyclists involved in these accidents than many other analogous datasets.  Datasets for individual years can be downloaded [here](https://pennshare.maps.arcgis.com/apps/webappviewer/index.html?id=8fdbf046e36e41649bbfd9d7dd7c7e7e).\n",
    "\n",
    "The PENNDOT datasets contain several different .csv files for each year on record.  Here is the full list, with the ones I'll use in bold:\n",
    "* COMMVEH: samples are commercial vehicles involved in crash incidents and features are specific to those types of vehicles\n",
    "* **CRASH: samples are crash incidents and features include a variety of information about the incident (e.g. when, where, how many and what type of people and vehicles were involved).**\n",
    "* **CYCLE: samples are pedal cycles and motorcycle vehicles involved in crash incidents and features are specific to those types of vehicles (e.g. helmet and safety device info, accessory info).**\n",
    "* **FLAG: samples are crash incidents and features are binary fields which flag additional factors in each incident (e.g. factors related to weather, other conditions, participant behavior, presence of fatality).  These were intended to help refine particular lookups in the dataset, and many of them can be subsumed by information provided in CRASH.**\n",
    "* **PERSON: samples are individuals involved in crash incidents, including cyclists, their passengers, and pedestrians and features are characteristics of those individuals (e.g. demographics, drug/alcohol results, position in vehicle)**\n",
    "* **ROADWAY: samples are roadways involved in crash incidents and features are supplementary roadway info (e.g. route number or name, posted speed limit)**\n",
    "* TRAILVEH: samples are trailers on vehicles involved in crash incidents and features are specific to trailers\n",
    "* **VEHICLE: samples are vehicles involved in crash incidents, including bicycles and pedestrians, and features are characteristic of those individuals (e.g. vehicle type and body style, vehicle movement and position)**\n",
    "\n",
    "Regarding identifiers:\n",
    "* Each sample in CRASH or FLAG should have a unique CRN (crash record number).\n",
    "* Each sample in CYCLE, VEHICLE should have a unique (CRN,UNIT_NUM) tuple, as UNIT_NUM is sequential among vehicles in each crash.\n",
    "* Each sample in PERSON should have a unique (CRN,UNIT_NUM,PERSON_NUM) tuple, as PERSON_NUM is sequential among persons in each vehicle unit.\n",
    "* Each CRN may correspond to many samples in ROADWAY.\n",
    "\n",
    "Here are links to two helpful documents provided by PENNDOT:\n",
    "* [Data dictionary](https://pennshare.maps.arcgis.com/sharing/rest/content/items/fd0412e19feb45419da49eb7a759060d/data)\n",
    "* [Additional document with more information](http://pennshare.maps.arcgis.com/sharing/rest/content/items/cae1501f58d842c385ebfaa963098d61/data)\n",
    "\n",
    "Note: the data dictionary defines \"serious injury\" and \"fatal injury\" as:\n",
    "* serious injury: \"incapacitating injury, including bleeding wounds and distorted members (amputations or broken bones), and requires transport of the patient from the scene.\"\n",
    "* fatal injury: \"the person dies as a result of the injuries sustaines in the crash within 30 days of the crash.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66dcac8-311b-45e5-a2e7-6ef584813e92",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part I: Obtaining cleaning, and pre-processing the dataset\n",
    "\n",
    "In this first notebook, I will do the following:\n",
    "1. Unzip individual year data files,\n",
    "2. Concatenate across years to build datasets for the entire period of 2002-2021.\n",
    "3. Perform some initial data cleaning and merging.\n",
    "4. Export the results to parquet files.\n",
    "\n",
    "First I import the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb088cd9-1426-487e-ac5b-3783c91abed4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import requests\n",
    "import json\n",
    "import glob\n",
    "import zipfile\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth',None)\n",
    "\n",
    "import sys\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e74f617-73ca-4be8-aaf0-fe1627935e63",
   "metadata": {},
   "source": [
    "### Unzip crash data files from individual years\n",
    "\n",
    "The following block unzips a collection of ZIP files downloaded from [this PENNDOT page.](https://pennshare.maps.arcgis.com/apps/webappviewer/index.html?id=8fdbf046e36e41649bbfd9d7dd7c7e7e)\n",
    "\n",
    "**These files are not included in the GitHub repository, due to their file sizes.** If you want to try out this process, you'll need to go download them from the PENNDOT page yourself.  Your ZIP files should be in the directory **'./data/zip/'** and the extracted CSV filed will end up in **'./data/raw_csv/'**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f2dda99-6fa9-48bc-b255-d9496e5f07e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# zip_files = glob.glob('data/zip/*.zip')\n",
    "# for file in zip_files:\n",
    "#     print(f'Unzipping {file}...')\n",
    "#     with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "#         zip_ref.extractall('data/raw_csv/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e866202c-f257-49db-80df-2ffcb6e0cefd",
   "metadata": {},
   "source": [
    "### Preparing dataframes for each year\n",
    "\n",
    "The get_data function will produce for a given year the following dataframes:\n",
    "* 'bicycles', a dataframe containing samples from the VEHICLE dataset which are bicycles (or other pedal cycles)\n",
    "* 'persons', a dataframe containing samples from the PERSON dataset whose CRN appears in 'bicycles', and which is supplemented with some relevant features from CYCLE\n",
    "* 'crashes', a dataframe containing all samples from the CRASH dataset whose CRN appears in 'bicycles', and which is supplemented with some relevant features from FLAG\n",
    "* 'roadway', a dataframe containing samples from the ROADWAY dataset whose CRN appears in 'bicycles'\n",
    "\n",
    "**Unless you want to download the PENNDOT files yourself and try this out, you can also skip the next block and load in my prepared raw CSV files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "390a8f69-feb5-4a2b-bc7f-2531b3bd5a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from lib.get_data import extract_data\n",
    "# bicycle_list, person_list, crash_list, roadway_list= [],[],[],[]\n",
    "\n",
    "# # Call dataframe collection function for each year\n",
    "# YEARS = range(2002,2022)\n",
    "# for year in YEARS:\n",
    "#     bicycles, persons,crashes,roadway= extract_data(year)\n",
    "#     bicycle_list.append(bicycles)\n",
    "#     person_list.append(persons)\n",
    "#     crash_list.append(crashes)\n",
    "#     roadway_list.append(roadway)\n",
    "\n",
    "\n",
    "# # # Concatenate yearly dataframes and reindex\n",
    "# bicycles = pd.concat(bicycle_list,axis=0).reset_index(drop=True)\n",
    "# persons = pd.concat(person_list,axis=0).reset_index(drop=True)\n",
    "# crashes = pd.concat(crash_list,axis=0).reset_index(drop=True)\n",
    "# roadway = pd.concat(roadway_list,axis=0).reset_index(drop=True)\n",
    "\n",
    "# del bicycle_list, person_list, crash_list\n",
    "\n",
    "# # # Write to CSV files\n",
    "# bicycles.to_csv('./data/raw_csv/bicycles_raw.csv',index=False)\n",
    "# persons.to_csv('./data/raw_csv/persons_raw.csv',index=False)\n",
    "# crashes.to_csv('./data/raw_csv/crashes_raw.csv',index=False)\n",
    "# roadway.to_csv('./data/raw_csv/roadway_raw.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c59ed6b-47c0-42d6-8cce-48485b25faa8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Start here to load in prepared CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec883c7e-bc5d-4430-a616-d237a64ba95b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bicycles = pd.read_csv('./data/raw_csv/bicycles_raw.csv')\n",
    "persons = pd.read_csv('./data/raw_csv/persons_raw.csv')\n",
    "crashes = pd.read_csv('./data/raw_csv/crashes_raw.csv')\n",
    "roadway = pd.read_csv('./data/raw_csv/roadway_raw.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b981b247-25f8-48ba-8b0e-428a115fed26",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Feature recoding and cleanup\n",
    "\n",
    "A few tasks are needed regarding the dataframes\n",
    "'bicycles', 'crashes', and 'persons':\n",
    "\n",
    "1. Address inconsistencies with respect to data types within columns\n",
    "2. Change 'unknown', 'other' and some non-sensical values to np.nan\n",
    "3. Recode categorical features to be more descriptive (see the [data dictionary](https://pennshare.maps.arcgis.com/sharing/rest/content/items/fd0412e19feb45419da49eb7a759060d/datahttps://pennshare.maps.arcgis.com/sharing/rest/content/items/fd0412e19feb45419da49eb7a759060d/data))\n",
    "4. Create from IMPACT_POINT (which uses clock number directions) a simpler feature IMPACT_SIDE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fab2d10-2ff8-4dc6-b3a3-7e4d984c5857",
   "metadata": {},
   "source": [
    "#### Features in 'bicycles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21da20f6-3019-4ec2-8735-02460b5d2e2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for feat in ['RDWY_ALIGNMENT','VEH_MOVEMENT','VEH_TYPE','VEH_POSITION','VEH_ROLE']:\n",
    "    bicycles[feat] = pd.to_numeric(bicycles[feat],errors='coerce')\n",
    "\n",
    "for feat in ['PC_HDLGHT_IND','PC_HLMT_IND','PC_REAR_RFLTR_IND']:\n",
    "    bicycles[feat]=bicycles[feat].replace({'Y':1,'N':0,'U':np.nan})\n",
    "    \n",
    "for feat in ['UNIT_NUM','IMPACT_POINT']:\n",
    "    bicycles[feat]=bicycles[feat].replace({'U':np.nan,99:np.nan})\n",
    "    \n",
    "bicycles['GRADE'] = bicycles['GRADE'].replace({1:'level',2:'uphill',3:'downhill',\n",
    "                                               4:'bottom_hill',5:'top_hill',9:np.nan})\n",
    "bicycles['IMPACT_SIDE'] = bicycles['IMPACT_POINT'].replace({0:'non_collision',13:'top',14:'undercarriage',\n",
    "                                                            11:'front',12:'front',1:'front',\n",
    "                                                            2:'front_right',3:'right',4:'rear_right',\n",
    "                                                            5:'rear',6:'rear',7:'rear',\n",
    "                                                            8:'rear_left',9:'left',10:'front_left',99:np.nan})\n",
    "bicycles['RDWY_ALIGNMENT'] = bicycles['RDWY_ALIGNMENT'].replace({1:'straight',2:'curve',\n",
    "                                                                 3:'curve',4:'curve',9:np.nan})\n",
    "\n",
    "bicycles['VEH_ROLE'] = bicycles['VEH_ROLE'].replace({0:'non_collision',1:'striking',2:'struck',3:'striking_struck'})\n",
    "bicycles['VEH_TYPE'] = bicycles['VEH_TYPE'].replace({20:'bicycle',21:'other_pedalcycle'})\n",
    "bicycles['VEH_POSITION'] = bicycles['VEH_POSITION'].replace({0:np.nan,1:'right_lane_curb',2:'right_lane',\n",
    "                                                            3:'left_lane',4:'left_turn_lane',\n",
    "                                                            5:'center_turn_lane',6:'other_forward_lane',\n",
    "                                                            7:'oncoming_lane',8:'left_of_trafficway',\n",
    "                                                            9:'right_of_trafficway',10:'HOV_lane',\n",
    "                                                            11:'shoulder_right',12:'shoulder_left',\n",
    "                                                            13:'one_lane_road',14:'acc_dec_lane',\n",
    "                                                            98:'other',99:np.nan})\n",
    "bicycles['VEH_MOVEMENT'] = bicycles['VEH_MOVEMENT'].replace({1:'straight',2:'slowing_or_stopping_in_lane',\n",
    "                                                            3:'stopped_in_lane',4:'passing_vehicle',\n",
    "                                                            5:'entering_lane',6:'parked',\n",
    "                                                            7:'leaving_lane',8:'avoiding',\n",
    "                                                            9:'turning_right_red',10:'turning_right',\n",
    "                                                            11:'turning_left_red',12:'turning_left',\n",
    "                                                            13:'u_turn',14:'backing',15:'changing_merging',\n",
    "                                                            16:'curve_right',17:'curve_left',\n",
    "                                                            18:'entering_lane',19:'leaving_lane',\n",
    "                                                            98:'other',99:np.nan})\n",
    "\n",
    "# One sample has missing UNIT_NUM, and - set it to first avail unit number\n",
    "bicycles['UNIT_NUM'] = bicycles.UNIT_NUM.fillna(1)\n",
    "\n",
    "# Two samples have missing VEH_ROLE, but they have IMPACT_POINT=='non_collision' - use that for VEH_ROLE\n",
    "bicycles['VEH_ROLE'] = bicycles.VEH_ROLE.fillna('non_collision')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4a6cdb-f6d7-42ff-bd31-7e64a71669d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Features in 'persons'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d0eb5e3-8f06-437a-b10a-26ac73b8f172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "persons['INJ_SEVERITY'] = pd.to_numeric(persons['INJ_SEVERITY'],errors='coerce')\n",
    "persons['AGE']=persons['AGE'].replace({99:np.nan})\n",
    "for feat in ['SEX','TRANSPORTED','UNIT_NUM']:\n",
    "    persons[feat]=persons[feat].replace({'Y':1,'N':0,'U':np.nan,'0':0,' ':np.nan,'R':np.nan,'9':np.nan,97:np.nan})\n",
    "persons['INJ_SEVERITY']=persons['INJ_SEVERITY'].replace({0:'no_injury',1:'killed',2:'susp_serious_injury',\n",
    "                                                         3:'susp_minor_injury',4:'possible_injury',\n",
    "                                                         8:'unknown_injury',9:np.nan})\n",
    "persons['PERSON_TYPE']=persons['PERSON_TYPE'].replace({1:'driver',2:'passenger',7:'pedestrian',\n",
    "                                                      8:'other',9:np.nan})\n",
    "persons['RESTRAINT_HELMET']=persons['RESTRAINT_HELMET'].replace({0:'no_restraint',1:'shoulder_belt',2:'lap_belt',\n",
    "                                                                3:'lap_shoulder_belt',4:'child_seat',\n",
    "                                                                5:'motorcycle_helmet',6:'bicycle_helmet',\n",
    "                                                                10:'belt_improper',11:'child_seat_improper',\n",
    "                                                                12:'helmet_improper',21:'child_seat',22:'child_seat',\n",
    "                                                                23:'booster_seat',24:'child_restraint_unknown',\n",
    "                                                                90:'unknown_restraint',98:'other',99:np.nan})\n",
    "# Two persons have CRN==2012109815 and UNIT_NUM==1.  Since the bicycle has UNIT_NUM==1, change UNIT_NUM to 2 for person who appears to be motorist\n",
    "persons.at[35557,'UNIT_NUM']=2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765bba30-62a0-40b6-b017-eeb63b3a2811",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Features in 'crashes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35d0f803-5b3a-4299-918b-125056bc262b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for feat in ['WEATHER1','WEATHER2','TIME_OF_DAY']:\n",
    "    crashes[feat] = pd.to_numeric(crashes[feat],errors='coerce')\n",
    "\n",
    "for feat in ['DISPATCH_TM','ARRIVAL_TM','TIME_OF_DAY']:\n",
    "    crashes[feat] = crashes[feat].replace({9999:np.nan})\n",
    "    \n",
    "crashes['HOUR_OF_DAY'] = crashes['HOUR_OF_DAY'].replace({99:np.nan,24:0})\n",
    "    \n",
    "crashes['COLLISION_TYPE'] = crashes['COLLISION_TYPE'].replace({0:'non_collision',1:'rear_end',\n",
    "                                                               2:'head_on',3:'backing',4:'angle',\n",
    "                                                               5:'sideswipe_same_dir',6:'sideswipe_opp_dir',\n",
    "                                                               7:'hit_fixed_obj',8:'hit_ped',\n",
    "                                                               9:'other',98:'other',99:np.nan})\n",
    "crashes['ILLUMINATION'] = crashes['ILLUMINATION'].replace({1:'daylight',2:'dark_unlit',3:'dark_lit',4:'dusk',\n",
    "                                                           5:'dawn',6:'dark_lit',8:'other',9:np.nan})\n",
    "crashes['ROAD_CONDITION'] = crashes['ROAD_CONDITION'].replace({1:'dry',2:'ice_frost',3:'mud_dirt_gravel',\n",
    "                                                              4:'oil',5:'sand',6:'slush',7:'snow',\n",
    "                                                              8:'water',9:'wet',98:'other',0:np.nan,99:np.nan})\n",
    "crashes['URBAN_RURAL'] = crashes['URBAN_RURAL'].replace({1:'rural',2:'urbanized',3:'urban'})\n",
    "crashes['INTERSECT_TYPE'] = crashes['INTERSECT_TYPE'].replace({0:'midblock',1:'four_way',2:'T',3:'Y',\n",
    "                                                               4:'circle',5:'multi_leg',6:'ramp_end',\n",
    "                                                               7:'ramp_begin',8:'crossover',9:'rr_crossing',\n",
    "                                                               10:'other',11:'L',12:'circle',13:'circle',99:np.nan})\n",
    "crashes['LOCATION_TYPE'] = crashes['LOCATION_TYPE'].replace({0:'not_applicable', 99:np.nan,1:'underpass',\n",
    "                                                            2:'ramp',3:'bridge',4:'tunnel',\n",
    "                                                            5:'toll_booth',6:'cross_over_related',\n",
    "                                                            7:'driveway_parking_lot', 8:'ramp_bridge'})\n",
    "crashes['RELATION_TO_ROAD'] = crashes['RELATION_TO_ROAD'].replace({1:'on_roadway',2:'shoulder', 3:'median',\n",
    "                                                                   4:'roadside',5:'outside_trafficway',\n",
    "                                                                   6:'parking_lane',7:'int_ramp_hwy',9:np.nan})\n",
    "crashes['TCD_TYPE'] = crashes['TCD_TYPE'].replace({0:'not_applicable',1:'flashing_traffic_signal',2:'traffic_signal',\n",
    "                                                 3:'stop_sign',4:'yield_sign',5:'active_RR_controls',6:'passive_RR_controls',\n",
    "                                                 7:'officer_or_flagman',8:'other',9:np.nan})\n",
    "\n",
    "crashes['TCD_FUNC_CD'] = crashes['TCD_FUNC_CD'].replace({0:'no_controls',1:'not_functioning',2:'functioning_improperly',\n",
    "                                                       3:'functioning_properly',4:'emergency_preemptive_signal',9:np.nan})\n",
    "\n",
    "for feat in ['WEATHER1','WEATHER2']:\n",
    "    crashes[feat] = pd.to_numeric(crashes[feat],errors='coerce')\n",
    "\n",
    "# Replace counts features for buses, trucks, SUVs with binary features indicating presence\n",
    "for feat in ['BUS','HEAVY_TRUCK','SMALL_TRUCK','SUV','VAN']:\n",
    "    crashes[feat] = crashes[feat+'_COUNT'].apply(lambda x: 1 if x>0 else x)\n",
    "    # crashes = crashes.drop(feat+'_COUNT',axis=1)\n",
    "\n",
    "# Recode 99 as np.nan\n",
    "for feat in ['WEATHER1','WEATHER2']:\n",
    "    crashes[feat]=crashes[feat].replace({99:np.nan})\n",
    "\n",
    "# fill empty WEATHER1 with WEATHER2 when possible\n",
    "crashes['WEATHER1'] = crashes.WEATHER1.fillna(crashes.WEATHER2)\n",
    "\n",
    "# if WEATHER1==WEATHER2, set WEATHER2 to be np.nan\n",
    "crashes.loc[crashes.WEATHER1==crashes.WEATHER2,'WEATHER2']=np.nan\n",
    "\n",
    "# If WEATHER1==3 (clear) and WEATHER2 is not np.nan, replace WEATHER1 with WEATHER2\n",
    "crashes['WEATHER1'] = crashes['WEATHER1'].where((crashes.WEATHER1!=3)|(crashes.WEATHER2.isna()),crashes.WEATHER2)\n",
    "\n",
    "# Rename and delete WEATHER2\n",
    "crashes = crashes.rename(columns={'WEATHER1':'WEATHER'})\n",
    "crashes = crashes.drop(columns='WEATHER2')\n",
    "\n",
    "crashes['WEATHER'] = crashes['WEATHER'].replace({1:'blowing_sand_soil_dirt',2:'blowing_snow', 3:'clear',\n",
    "                                                4:'cloudy',5:'fog_smog_smoke',6:'freezing_rain',\n",
    "                                                7:'rain',8:'severe_crosswind',9:'sleet_hail',\n",
    "                                                10:'snow',98:'other'})\n",
    "\n",
    "# Adjust incorrect DEC_LAT, DEC_LONG\n",
    "\n",
    "# This sample missing fractional part\n",
    "crashes.at[24770,'DEC_LAT']=np.nan\n",
    "crashes.at[24770,'DEC_LONG']=np.nan\n",
    "\n",
    "# These samples locations don't match minicipalities - replace with approx location based on roadway data\n",
    "crashes.at[3087,'DEC_LAT']=40.0081\n",
    "crashes.at[3087,'DEC_LONG']=-75.1923\n",
    "crashes.at[24805,'DEC_LAT']=40.3322\n",
    "crashes.at[24805,'DEC_LONG']=-75.9278\n",
    "\n",
    "# Fill NaN lat,lon values with mean by municipality when possible, then fill remaining with mean by county when possible.\n",
    "for coord in ['DEC_LAT','DEC_LONG']:\n",
    "    for area in ['MUNICIPALITY','COUNTY']:\n",
    "        crashes[coord]=crashes.groupby(area)[coord].transform(lambda x:x.fillna(x.mean()))\n",
    "        \n",
    "# Fill some missing HOUR_OF_DAY using TIME_OF_DAY, or DISPATCH_TM, or ARRIVAL_TM when possible\n",
    "crashes['HOUR_OF_DAY'] = crashes.HOUR_OF_DAY.fillna(crashes.TIME_OF_DAY.floordiv(100))\n",
    "crashes['HOUR_OF_DAY'] = crashes.HOUR_OF_DAY.fillna(crashes.DISPATCH_TM.floordiv(100))\n",
    "crashes['HOUR_OF_DAY'] = crashes.HOUR_OF_DAY.fillna(crashes.ARRIVAL_TM.floordiv(100))\n",
    "\n",
    "# Fill all five missing ILLUMINATION 'daylight'\n",
    "crashes['ILLUMINATION'] = crashes.ILLUMINATION.fillna('daylight')\n",
    "\n",
    "# Don't need ARRIVAL_TM, DISPATCH_TM anymore\n",
    "crashes = crashes.drop(columns=['DISPATCH_TM','ARRIVAL_TM'])\n",
    "\n",
    "# Fill missing SPEED_LIMIT with mode by county.\n",
    "roadway['SPEED_LIMIT'] = roadway.groupby('RDWY_COUNTY',sort=False)['SPEED_LIMIT'].apply(lambda x: x.fillna(x.mode()[0])).astype('int')\n",
    "\n",
    "# When there are several roadways with the same CRN, don't know which one the cyclist was on.\n",
    "# Use minimum value among all roadway samples with matching CRN\n",
    "crashes = crashes.merge(pd.DataFrame(roadway.groupby('CRN').min().SPEED_LIMIT),how='left',on='CRN')\n",
    "del roadway"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6aa17c-de59-4dd1-91f1-7e91c4488a37",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Creating merged 'cyclists' dataframe\n",
    "\n",
    "It will be useful later to have a dataframe containing only cyclists as samples, and which also have vehicle and crash features.  I accomplish this by merging some dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01f11d67-171f-43eb-96ea-143c75cc32b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge bicycle vehicle data onto persons and crashes\n",
    "cols = ['CRN', 'UNIT_NUM','INJ_SEVERITY','PERSON_TYPE',\n",
    "        'AGE','SEX','RESTRAINT_HELMET']\n",
    "cyclists = persons[cols].merge(bicycles,on=['CRN','UNIT_NUM'],how='left')\n",
    "\n",
    "# Isolate cyclists by restricting to samples which inherit VEH_TYPE from bicycles\n",
    "cyclists = cyclists[cyclists.VEH_TYPE.notnull()]\n",
    "\n",
    "# Merge crash data onto cyclists\n",
    "\n",
    "cols = ['CRN', 'COUNTY', 'MUNICIPALITY', 'BUS_COUNT','COMM_VEH_COUNT',\n",
    "       'HEAVY_TRUCK_COUNT', 'SMALL_TRUCK_COUNT', 'SUV_COUNT', 'VAN_COUNT',\n",
    "       'CRASH_MONTH', 'CRASH_YEAR', 'DAY_OF_WEEK', 'HOUR_OF_DAY',\n",
    "       'COLLISION_TYPE', 'ILLUMINATION', 'INTERSECT_TYPE', 'LOCATION_TYPE',\n",
    "       'RELATION_TO_ROAD', 'ROAD_CONDITION', 'TCD_TYPE', 'TCD_FUNC_CD',\n",
    "       'URBAN_RURAL', 'WEATHER', 'AGGRESSIVE_DRIVING', 'ANGLE_CRASH',\n",
    "       'CELL_PHONE', 'COMM_VEHICLE', 'CROSS_MEDIAN', 'CURVED_ROAD',\n",
    "        'CURVE_DVR_ERROR','DRUG_RELATED','ALCOHOL_RELATED',\n",
    "       'DISTRACTED', 'DRINKING_DRIVER', 'DRUGGED_DRIVER', 'FATIGUE_ASLEEP','HO_OPPDIR_SDSWP',\n",
    "       'ICY_ROAD', 'ILLUMINATION_DARK', 'IMPAIRED_DRIVER', 'INTERSECTION',\n",
    "       'LANE_DEPARTURE', 'NHTSA_AGG_DRIVING','NO_CLEARANCE','NON_INTERSECTION','REAR_END',\n",
    "       'RUNNING_RED_LT', 'RUNNING_STOP_SIGN', 'RURAL', 'SNOW_SLUSH_ROAD',\n",
    "       'SPEEDING', 'SPEEDING_RELATED', 'SUDDEN_DEER', 'TAILGATING', 'URBAN',\n",
    "       'WET_ROAD', 'WORK_ZONE', 'MATURE_DRIVER', 'YOUNG_DRIVER', 'BUS',\n",
    "       'HEAVY_TRUCK', 'SMALL_TRUCK', 'SUV', 'VAN', 'SPEED_LIMIT']\n",
    "\n",
    "cyclists = cyclists.merge(crashes[cols],on=['CRN'],how='left').drop('VEH_TYPE',axis=1)\n",
    "\n",
    "del cols, persons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9580b27f-c4a8-4e0f-bcb6-915fd01cf467",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Missing data in 'cyclists'\n",
    "\n",
    "Many fields have missing data which I would like to fill using medians or modes (either groupwise or global).  I will later design the BikeSafePA model to predict whether a cyclist suffers serious injury or fatality, so I will wait to implement this median/mode imputation as part of a pipeline to avoid data leakage from test set to training set.  However, here's the plan:\n",
    "\n",
    "* Create a binary SERIOUS_OR_FATALITY which is 1 if INJ_SEVERITY is in ['susp_serious_inj','killed'] and 0 otherwise (including NaN): can do this now\n",
    "* When only one of 'ROAD_CONDITION' or 'WEATHER' is missing, will sometimes use one to fill the other without relying on the sample distribution: can do this now\n",
    "* Impute 'AGE' with groupwise median, grouped by 'MUNICIPALITY': implement in pipeline later\n",
    "* Impute 'HOUR_OF_DAY' with groupwise mode, grouped by ('ILLUMINATION','CRASH_MONTH'): implement in pipeline later\n",
    "* Impute 'RESTRAINT_HELMET', 'IMPACT_SIDE', 'GRADE', 'VEH_POSITION' by creating a new category 'unknown': can do this now\n",
    "* Impute all other missing data using dataset mode for feature: implement in pipeline later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af0ac1b8-34c7-41ed-9d98-137bce1a1757",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# When ROAD_CONDITION is dry, it makes sense to fill WEATHER to clear\n",
    "cyclists.loc[(cyclists.ROAD_CONDITION=='dry')&(cyclists.WEATHER.isna()),'WEATHER'] = 'clear'\n",
    "\n",
    "# When WEATHER is rain, is makes sense to fill ROAD_CONDITION to wet\n",
    "cyclists.loc[(cyclists.WEATHER=='rain')&(cyclists.ROAD_CONDITION.isna()),'ROAD_CONDITION'] = 'wet'\n",
    "\n",
    "# These have many NaN - create new 'unknown' category\n",
    "for feature in ['RESTRAINT_HELMET', 'IMPACT_SIDE', 'GRADE', 'VEH_POSITION']:\n",
    "    cyclists[feature]=cyclists[feature].fillna('unknown')\n",
    "    \n",
    "# Create injury severity flag\n",
    "cyclists['SERIOUS_OR_FATALITY'] = cyclists['INJ_SEVERITY']\\\n",
    "                                .apply(lambda x: 1 if x in ['killed','susp_serious_injury'] else 0)\n",
    "# Also bin the age values\n",
    "cyclists['AGE_BINS'] = pd.cut(cyclists.AGE,bins=range(0,101,10)).astype('str')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695b0d58-dd1e-49dd-9beb-aad69aa5cb23",
   "metadata": {},
   "source": [
    "### Exporting cleaned dataframes\n",
    "\n",
    "These will end up in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99fce163-6748-4e46-a3cb-fc30bef45c09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cyclists.to_csv('cyclists.csv',index=False)\n",
    "crashes.to_csv('crashes.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
